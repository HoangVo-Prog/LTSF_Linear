# M√î T·∫¢ ƒê·∫¶Y ƒê·ª¶ PIPELINE D·ª∞ B√ÅO GI√Å FPT 100 NG√ÄY B·∫∞NG TREND + RESIDUAL MODEL

To√†n b·ªô pipeline g·ªìm 6 kh·ªëi ch√≠nh:

1. **C·∫•u h√¨nh th√≠ nghi·ªám v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu g·ªëc**
2. **M√¥ h√¨nh xu h∆∞·ªõng d√†i h·∫°n (TrendModel)**
3. **X√¢y d·ª±ng ƒë·∫∑c tr∆∞ng k·ªπ thu·∫≠t v√† dataset supervised cho residual**
4. **Hu·∫•n luy·ªán residual model, ch·ªçn ƒë·∫∑c tr∆∞ng l√µi v√† t·ªëi ∆∞u hyperparameter**
5. **Forecast nhi·ªÅu b∆∞·ªõc (100 ng√†y) v√† ki·ªÉm tra tr√™n validation**
6. **D·ª± b√°o 100 ng√†y cu·ªëi c√πng v√† t·∫°o submission**

B√™n d∆∞·ªõi l√† m√¥ t·∫£ chi ti·∫øt t·ª´ng kh·ªëi, k√®m vai tr√≤, input, output v√† logic x·ª≠ l√Ω.

---

# 1. C·∫•u h√¨nh th√≠ nghi·ªám v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu g·ªëc

## 1.1. C·∫•u h√¨nh chung trong `config.py`

To√†n b·ªô tham s·ªë th√≠ nghi·ªám ƒë∆∞·ª£c t·∫≠p trung trong `config.py`:

* ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu:

  * `TRAIN_CSV`: file ch·ª©a d·ªØ li·ªáu l·ªãch s·ª≠ FPT
  * `SUBMISSION_TEMPLATE_CSV`: file template submission c·ªßa BTC
  * `OUTPUT_SUBMISSION_CSV`: t√™n file submission ƒë·∫ßu ra

* M·ªëc th·ªùi gian chia t·∫≠p:

  * `TRAIN_END_DATE`: m·ªçi quan s√°t c√≥ `time < TRAIN_END_DATE` thu·ªôc train, c√≤n l·∫°i thu·ªôc validation

* C·∫•u h√¨nh m√¥ h√¨nh:

  * `TREND_POLY_DEGREE`: b·∫≠c ƒëa th·ª©c c·ªßa m√¥ h√¨nh xu h∆∞·ªõng (TrendModel)
  * `RESIDUAL_MODEL_TYPE`: lo·∫°i model residual (`"elasticnet"` ho·∫∑c `"ridge"`)
  * `RESIDUAL_SHRINK`: h·ªá s·ªë thu nh·ªè residual trong qu√° tr√¨nh forecast nhi·ªÅu b∆∞·ªõc

* C·∫•u h√¨nh feature:

  * `MAX_LAG`: s·ªë lag t·ªëi ƒëa khi x√¢y supervised dataset cho residual
  * `RET_WINDOWS`: c√°c c·ª≠a s·ªï t√≠nh return (1, 5, 20 ng√†y)
  * `VOL_WINDOWS`: c√°c c·ª≠a s·ªï t√≠nh rolling vol
  * `SMA_WINDOWS`: c√°c c·ª≠a s·ªï SMA v√† c√°c ƒë·∫∑c tr∆∞ng li√™n quan

* C·∫•u h√¨nh feature selection v√† search:

  * `TOP_K_FEATURES`: s·ªë l∆∞·ª£ng feature l√µi gi·ªØ l·∫°i sau permutation importance
  * `N_RANDOM_SEARCH`: s·ªë trial random search cho hyperparameter residual model

* C·∫•u h√¨nh d·ª± b√°o:

  * `FORECAST_STEPS`: s·ªë ng√†y c·∫ßn d·ª± b√°o cho submission (100) 

---

## 1.2. Load d·ªØ li·ªáu, chu·∫©n h√≥a c∆° b·∫£n v√† log price

H√†m `load_price_data` trong `data_utils.py` th·ª±c hi·ªán:

1. ƒê·ªçc CSV `TRAIN_CSV`
2. Parse c·ªôt `time` sang ki·ªÉu datetime
3. S·∫Øp x·∫øp theo `time` v√† reset index
4. T·∫°o c·ªôt ch·ªâ s·ªë th·ªùi gian r·ªùi r·∫°c:

   * `t = index` d·∫°ng integer, d√πng l√†m input cho TrendModel
5. T√≠nh c·ªôt:

   * `log_price = log(close + 1e-8)`
     ƒê√¢y l√† target ch√≠nh ƒë·ªÉ m√¥ h√¨nh xu h∆∞·ªõng l√†m vi·ªác, ƒë·ªìng th·ªùi c≈©ng ƒë∆∞·ª£c d√πng trong feature k·ªπ thu·∫≠t. 

---

## 1.3. Chia train v√† validation theo m·ªëc ng√†y

H√†m `train_val_split` chia d·ªØ li·ªáu theo `TRAIN_END_DATE`:

* `train_df`: m·ªçi d√≤ng c√≥ `time < TRAIN_END_DATE`
* `val_df`: m·ªçi d√≤ng c√≥ `time >= TRAIN_END_DATE` 

Trong `main.py`, ƒëo·∫°n ƒë·∫ßu th·ª±c hi·ªán:

```python
df = load_price_data(TRAIN_CSV)
train_df, val_df = train_val_split(df, TRAIN_END_DATE)
```

Sau ƒë√≥ in ra:

* Kho·∫£ng th·ªùi gian c·ªßa train v√† validation
* S·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói ph·∫ßn

ƒê·ªìng th·ªùi t√≠nh:

* `price_min_hist = df["close"].min()`
* `price_max_hist = df["close"].max()`

V√† thi·∫øt l·∫≠p bi√™n clip cho log_price:

* `clip_low = log(0.9 * min_price_hist)`
* `clip_high = log(1.1 * max_price_hist)`

Bi√™n n√†y d√πng xuy√™n su·ªët trong forecast ƒë·ªÉ ch·∫∑n log_price kh√¥ng tr√¥i qu√° xa so v·ªõi l·ªãch s·ª≠. 

---

# 2. M√¥ h√¨nh xu h∆∞·ªõng d√†i h·∫°n (TrendModel)

## 2.1. √ù t∆∞·ªüng

Gi√° c·ªï phi·∫øu th∆∞·ªùng c√≥ xu h∆∞·ªõng d√†i h·∫°n t∆∞∆°ng ƒë·ªëi m∆∞·ª£t, c√≤n ph·∫ßn nhi·ªÖu ng·∫Øn h·∫°n l√† bi·∫øn ƒë·ªông quanh xu h∆∞·ªõng ƒë√≥. Pipeline t√°ch b√†i to√°n th√†nh hai t·∫ßng:

1. H·ªçc xu h∆∞·ªõng tr∆°n c·ªßa `log_price` theo th·ªùi gian b·∫±ng m√¥ h√¨nh ƒëa th·ª©c b·∫≠c th·∫•p
2. H·ªçc residual (log_price tr·ª´ trend) b·∫±ng m√¥ h√¨nh tuy·∫øn t√≠nh regularized (ElasticNet ho·∫∑c Ridge)

ƒêi·ªÅu n√†y gi√∫p residual model t·∫≠p trung m√¥ t·∫£ c·∫•u tr√∫c ng·∫Øn h·∫°n, thay v√¨ ph·∫£i g√°nh lu√¥n xu h∆∞·ªõng d√†i h·∫°n.

---

## 2.2. C√†i ƒë·∫∑t TrendModel

L·ªõp `TrendModel` trong `trend_model.py` g·ªìm:

* `PolynomialFeatures(degree)` v·ªõi `include_bias=True`
* `LinearRegression` ƒë·ªÉ fit tr√™n `log_price`

C·ª• th·ªÉ:

* Input: c·ªôt `t` (index th·ªùi gian)
* Bi·∫øn ƒë·ªïi `t` sang ma tr·∫≠n ƒëa th·ª©c `[1, t, t^2, ..., t^degree]`
* Fit LinearRegression tr√™n `log_price` 

---

## 2.3. Th√™m c·ªôt trend v√† residual v√†o DataFrame

Ph∆∞∆°ng th·ª©c `add_trend_and_residual`:

1. Copy `df`
2. D√πng `predict_on_index(df["t"].values)` ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng:

   * `trend_t = f(t)` tr√™n to√†n b·ªô l·ªãch s·ª≠
3. Th√™m hai c·ªôt:

   * `trend`
   * `resid = log_price - trend` 

Trong `main.py`, m√¥ h√¨nh xu h∆∞·ªõng ƒë∆∞·ª£c fit m·ªôt l·∫ßn tr√™n **to√†n b·ªô l·ªãch s·ª≠**:

```python
trend_model = TrendModel(degree=TREND_POLY_DEGREE)
trend_model.fit(df)
df_trend_all = trend_model.add_trend_and_residual(df)
``` 

ƒê√¢y l√† ƒëi·ªÉm quan tr·ªçng: xu h∆∞·ªõng d√†i h·∫°n ƒë∆∞·ª£c ∆∞·ªõc l∆∞·ª£ng b·∫±ng t·∫•t c·∫£ d·ªØ li·ªáu 2020 2025, sau ƒë√≥ d√πng chung cho m·ªçi ph·∫ßn sau.

---

# 3. X√¢y d·ª±ng ƒë·∫∑c tr∆∞ng k·ªπ thu·∫≠t v√† dataset supervised cho residual

## 3.1. Feature engineering v·ªõi `add_technical_features`

H√†m `add_technical_features` trong `features.py` nh·∫≠n v√†o `df` ƒë√£ c√≥:

- `time`, `t`, `log_price`, `close`, `volume`, `resid`, v.v.

v√† t·∫°o th√™m h·ªá th·ªëng ƒë·∫∑c tr∆∞ng k·ªπ thu·∫≠t, bao g·ªìm:

1. **Return v√† log return c∆° b·∫£n:**

   - `ret_1d = close.pct_change()`  
   - `log_ret_1d = diff(log_price)`  

   V√† cho m·ªói c·ª≠a s·ªï `w` trong `RET_WINDOWS`:

   - `log_ret_{w}d = log_price.diff(w)`  
   - `ret_{w}d = close.pct_change(w)`  

2. **Rolling volatility tr√™n log_ret:**

   V·ªõi m·ªói `w` trong `VOL_WINDOWS`:

   - `vol_{w}d = rolling_std(log_ret_1d, window=w)`  

3. **Simple Moving Average v√† t∆∞∆°ng quan v·ªõi SMA:**

   V·ªõi m·ªói `w` trong `SMA_WINDOWS`:

   - `sma_{w} = rolling_mean(close, window=w)`  
   - `price_sma_{w}_rel = close / sma_{w} - 1`  

4. **Volume features:**

   C≈©ng tr√™n m·ªói `w` trong `SMA_WINDOWS`:

   - `vol_ma_{w} = rolling_mean(volume, window=w)`  
   - `vol_rel_{w} = volume / vol_ma_{w} - 1`  

5. **RSI 14 phi√™n:**

   - D√πng `delta = close.diff()`  
   - T√°ch `up` v√† `down`, t√≠nh `roll_up = mean(up, window=14)` v√† `roll_down = mean(down, window=14)`  
   - `rs = roll_up / (roll_down + 1e-8)`  
   - `rsi_14 = 100 - 100 / (1 + rs)` 

K·∫øt qu·∫£: DataFrame m·ªü r·ªông `df_feat_all` ch·ª©a ƒë·∫ßy ƒë·ªß OHLCV, trend, resid v√† to√†n b·ªô technical features.

---

## 3.2. X√¢y d·ª±ng dataset supervised cho residual

H√†m `make_supervised_residual_dataset` t·∫°o ra dataset d·∫°ng supervised cho residual model:

- **Target:**  
  - `target_resid_next = resid.shift(-1)`  
  T·ª©c l√† residual c·ªßa **ng√†y ti·∫øp theo**  

- **Base feature list:**  
  B·∫Øt ƒë·∫ßu t·ª´ c√°c c·ªôt n·ªÅn:

```text
  ["resid", "log_price", "ret_1d", "log_ret_1d",
   log_ret_{w}d, ret_{w}d,
   vol_{w}d,
   sma_{w}, price_sma_{w}_rel, vol_ma_{w}, vol_rel_{w},
   rsi_14]
```

trong ƒë√≥ `w` ch·∫°y qua RET_WINDOWS, VOL_WINDOWS, SMA_WINDOWS, nh∆∞ng ch·ªâ gi·ªØ nh·ªØng c·ªôt th·ª±c s·ª± t·ªìn t·∫°i trong DataFrame. 

* **T·∫°o lag:**

  V·ªõi m·ªói c·ªôt `col` trong `base_cols`, v√† m·ªói `lag` t·ª´ `0` ƒë·∫øn `MAX_LAG`, t·∫°o feature:

  * `cname = f"{col}_lag{lag}" = df[col].shift(lag)`

  T·ª©c l√† m·ªói th√¥ng tin ƒë∆∞·ª£c nh√¨n qua nhi·ªÅu lag th·ªùi gian, gi√∫p model n·∫Øm ƒë·ªông l·ª±c g·∫ßn ƒë√¢y.

* **K·∫øt qu·∫£ cu·ªëi:**

  * `X`: ma tr·∫≠n feature g·ªìm t·∫•t c·∫£ c·ªôt `{base_feature}_lag{0..MAX_LAG}`
  * `y`: series `target_resid_next`
  * `feature_names`: list t√™n c·ªôt t∆∞∆°ng ·ª©ng

Sau ƒë√≥:

* Lo·∫°i b·ªè m·ªçi d√≤ng c√≥ NaN ·ªü X ho·∫∑c y
* Tr·∫£ v·ªÅ `(X, y, feature_names)` 

Trong `main.py`, pipeline x√¢y supervised dataset cho **to√†n b·ªô l·ªãch s·ª≠**:

```python
df_trend_all = trend_model.add_trend_and_residual(df)
df_feat_all = add_technical_features(df_trend_all)
X_all, y_all, feature_names_raw = make_supervised_residual_dataset(df_feat_all)
```

Sau ƒë√≥ d√πng c·ªôt `time` t∆∞∆°ng ·ª©ng v·ªõi c√°c index c·ªßa `X_all` ƒë·ªÉ t√°ch ra supervised train v√† val:

* `train_mask = time < TRAIN_END_DATE`
* `val_mask = time >= TRAIN_END_DATE` 

---

# 4. Hu·∫•n luy·ªán residual model, ch·ªçn ƒë·∫∑c tr∆∞ng l√µi v√† t·ªëi ∆∞u hyperparameter

## 4.1. ResidualModel v√† chu·∫©n h√≥a

`ResidualModel` l√† m·ªôt wrapper che ph·ªß:

* `ElasticNet` ho·∫∑c `Ridge` t·ª´ sklearn
* M·ªôt `StandardScaler` ƒë·ªÉ chu·∫©n h√≥a X

ƒê·∫∑c ƒëi·ªÉm:

* Khi `fit(X, y)`:

  * N·∫øu `use_scaler=True`: fit scaler tr√™n X, transform th√†nh Xs
  * Fit model g·ªëc tr√™n Xs
* Khi `predict(X)`:

  * Transform X qua scaler (n·∫øu c√≥)
  * Predict tr√™n Xs
* `score_mse(X, y)`: tr·∫£ v·ªÅ MSE gi·ªØa y v√† y_pred

Model g·ªëc ƒë∆∞·ª£c ƒë·∫∑t trong `_base_model`, truy c·∫≠p qua property `.model`. 

---

## 4.2. Fit base residual model ƒë·ªÉ t√≠nh feature importance

Trong `main.py`:

1. T√°ch supervised train v√† val:

   ```python
   X_train = supervised_df.loc[train_mask, used_feature_names]
   y_train = supervised_df.loc[train_mask, "target"]
   X_val   = supervised_df.loc[val_mask, used_feature_names]
   y_val   = supervised_df.loc[val_mask, "target"]
   ```

2. Kh·ªüi t·∫°o model n·ªÅn:

   * N·∫øu `RESIDUAL_MODEL_TYPE == "elasticnet"`:

     * `alpha=0.01`, `l1_ratio=0.5`, `max_iter=10000`
   * Ng∆∞·ª£c l·∫°i d√πng `Ridge(alpha=1.0)`

3. Fit tr√™n `(X_train, y_train)` v√† t√≠nh MSE tr√™n train v√† val ƒë·ªÉ l√†m baseline. 

---

## 4.3. Permutation importance v√† ch·ªçn top K feature

ƒê·ªÉ model forecast 100 ng√†y ·ªïn ƒë·ªãnh h∆°n, ta kh√¥ng d√πng to√†n b·ªô feature m√† ch·ªçn ra m·ªôt t·∫≠p ƒë·∫∑c tr∆∞ng l√µi.

Pipeline:

1. D√πng `compute_feature_importance(base_model, X_val, y_val)`:

   * `permutation_importance` ·ªü sklearn ho√°n v·ªã t·ª´ng c·ªôt feature nhi·ªÅu l·∫ßn
   * ƒêo m·ª©c gi·∫£m performance MSE, t·ª´ ƒë√≥ t√≠nh mean importance cho m·ªói feature
   * Tr·∫£ v·ªÅ `importances` d·∫°ng `np.ndarray` ƒë·ªô d√†i = s·ªë feature 

2. L·∫•y `feature_names_val = list(X_val.columns)`

3. D√πng `select_top_k_features(importances, feature_names_val, TOP_K_FEATURES)`:

   * Sort index `idx_sorted = argsort(importances)[::-1]`
   * Ch·ªçn `k` index ƒë·ª©ng ƒë·∫ßu
   * Tr·∫£ v·ªÅ list `top_features` t∆∞∆°ng ·ª©ng

4. Lo·∫°i b·ªè m·ªçi feature b·∫Øt ƒë·∫ßu b·∫±ng `"resid_lag"`:

   ```python
   top_features = [f for f in top_features if not f.startswith("resid_lag")]
   ```

   L√Ω do: n·∫øu d√πng tr·ª±c ti·∫øp c√°c lag c·ªßa residual, forecast nhi·ªÅu b∆∞·ªõc r·∫•t d·ªÖ t√≠ch l≈©y sai s·ªë v√† diverge.

5. N·∫øu sau khi lo·∫°i resid_lag m√† kh√¥ng c√≤n feature n√†o:

   * T√¨m t·∫≠p index kh√¥ng ph·∫£i residual
   * Ch·ªçn 20 feature kh√¥ng ph·∫£i resid_lag c√≥ importance cao nh·∫•t l√†m fallback. 

---

## 4.4. T·ªëi ∆∞u hyperparameter b·∫±ng random search

Sau khi ch·ªçn core feature, pipeline:

1. L·ªçc l·∫°i X:

   ```python
   X_train_core = filter_feature_matrix(X_train, top_features)
   X_val_core   = filter_feature_matrix(X_val, top_features)
   ```

2. N·∫øu m√¥ h√¨nh l√† ElasticNet:

   * G·ªçi `random_search_elasticnet(X_train_core, y_train, X_val_core, y_val, n_trials=N_RANDOM_SEARCH)`

   Trong m·ªói trial:

   * Sample:

     * `alpha` t·ª´ log-uniform trong [1e‚àí4, 1]
     * `l1_ratio` t·ª´ uniform trong [0, 1]
   * Fit ResidualModel v·ªõi hyper n√†y tr√™n train
   * T√≠nh `mse_val`
   * C·∫≠p nh·∫≠t `best_model`, `best_params` n·∫øu t·ªët h∆°n

3. N·∫øu m√¥ h√¨nh l√† Ridge:

   * G·ªçi `random_search_ridge` v·ªõi:

     * `alpha` trong [1e‚àí4, 1e2] theo log-uniform

K·∫øt qu·∫£: m·ªôt `best_model` tr√™n core features v√† dict `best_params`. 

---

## 4.5. Refit final residual model tr√™n to√†n b·ªô supervised data

ƒê·ªÉ s·ª≠ d·ª•ng t·ªëi ƒëa d·ªØ li·ªáu, sau random search, code:

1. T·∫°o `X_all_core = filter_feature_matrix(X_all, top_features)`

2. L·∫•y `y_all_core = y_all.loc[X_all_core.index]`

3. Kh·ªüi t·∫°o `final_residual_model` v·ªõi:

   * `model_type = best_model.model_type`
   * C√°c hyperparameter l·∫•y t·ª´ `best_params`:

     * `alpha`, `l1_ratio` (n·∫øu ElasticNet) ho·∫∑c `alpha` (n·∫øu Ridge)

4. Fit `final_residual_model.fit(X_all_core, y_all_core)` tr√™n to√†n b·ªô supervised dataset. 

K·∫øt qu·∫£: m·ªôt residual model m·∫°nh nh·∫•t c√≥ th·ªÉ, d√πng to√†n b·ªô d·ªØ li·ªáu qu√° kh·ª© v√† t·∫≠p ƒë·∫∑c tr∆∞ng l√µi ·ªïn ƒë·ªãnh.

---

# 5. Forecast nhi·ªÅu b∆∞·ªõc (100 ng√†y) v√† ki·ªÉm tra tr√™n validation

## 5.1. H√†m `forecast_future_prices`: recursive multi step forecast

H√†m n√†y hi·ªán th·ª±c logic d·ª± b√°o nhi·ªÅu b∆∞·ªõc trong `forecast.py`:

Input ch√≠nh:

* `df_hist_raw`: l·ªãch s·ª≠ gi√° th·ª±c ƒë·∫øn th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu forecast
* `trend_model`: TrendModel ƒë√£ fit
* `residual_model`: final ResidualModel ƒë√£ hu·∫•n luy·ªán
* `feature_names`: danh s√°ch top_features l√µi
* `steps`: s·ªë b∆∞·ªõc d·ª± b√°o (v√≠ d·ª• 100)
* `log_clip_low`, `log_clip_high`: bi√™n clip cho log_price
* `residual_shrink`: h·ªá s·ªë thu nh·ªè residual (t·ª´ config) 

Quy tr√¨nh cho m·ªói b∆∞·ªõc forecast:

1. **Chu·∫©n h√≥a history:**

   * S·∫Øp x·∫øp `df_hist` theo `time` v√† reset index
   * T·∫°o l·∫°i `t = index`
   * T√≠nh l·∫°i `log_price = log(close + 1e-8)`

2. **Th√™m trend v√† residual, build feature:**

   * D√πng `trend_model.add_trend_and_residual(df_hist)` ƒë·ªÉ t·∫°o `trend` v√† `resid`
   * G·ªçi `add_technical_features` ƒë·ªÉ th√™m to√†n b·ªô technical features

3. **T·∫°o supervised residual dataset:**

   * G·ªçi `make_supervised_residual_dataset(df_feat)`
   * Thu ƒë∆∞·ª£c `X_all`, `y_all`, `all_feat_names`

4. **Gi·ªØ l·∫°i core features:**

   * `X_all = filter_feature_matrix(X_all, feature_names)`

5. **D·ª± b√°o residual ti·∫øp theo:**

   * L·∫•y d√≤ng cu·ªëi c√πng `x_latest = X_all.iloc[[-1]]`
   * `resid_pred = residual_model.predict(x_latest)`
   * `resid_next = residual_shrink * resid_pred`

6. **T√°i d·ª±ng log_price k·∫ø ti·∫øp:**

   * `last_log_price = df_hist["log_price"].iloc[-1]`
   * Trend anchor ƒë∆∞·ª£c ch·ªçn ch√≠nh l√† gi√° tr·ªã log_price cu·ªëi c√πng n√†y
   * `log_price_next = last_log_price + resid_next`
   * Clip `log_price_next` v√†o `[log_clip_low, log_clip_high]`

7. **Chuy·ªÉn v·ªÅ price v√† append v√†o history:**

   * `price_next = exp(log_price_next)`
   * T·∫°o m·ªôt d√≤ng m·ªõi v·ªõi:

     * `time = last_time + 1 day`
     * `open = high = low = close = price_next`
     * `volume` v√† `symbol` ƒë∆∞·ª£c l·∫•y t·ª´ d√≤ng cu·ªëi c√πng c·ªßa history
   * Append v√†o `df_hist` v√† l∆∞u `price_next` v√†o list `preds`

8. L·∫∑p l·∫°i b∆∞·ªõc 1 7 `steps` l·∫ßn. 

K·∫øt qu·∫£: m·∫£ng `preds` ch·ª©a ƒë∆∞·ªùng gi√° forecast ƒëa b∆∞·ªõc.

---

## 5.2. ƒê√°nh gi√° path MSE 100 ng√†y tr√™n validation

ƒê·ªÉ ki·ªÉm tra kh·∫£ nƒÉng d·ª± b√°o ƒëa b∆∞·ªõc c·ªßa pipeline, `main.py` c√≥ h√†m:

```python
evaluate_path_mse_on_validation(
    df_full=df,
    trend_model=trend_model,
    residual_model=final_residual_model,
    feature_names=top_features,
    start_date="2024-01-02",
    horizon=100,
    log_clip_low=clip_low,
    log_clip_high=clip_high,
)
```

Logic c·ªßa `evaluate_path_mse_on_validation`:

1. `df_hist = df_full[time <= start_date]` l√†m history  
2. `df_future_true = df_full[time > start_date]` s·∫Øp x·∫øp theo th·ªùi gian, reset index  
3. G·ªçi `forecast_future_prices` v·ªõi `steps = horizon` ƒë·ªÉ d·ª± b√°o 100 ng√†y ti·∫øp theo t·ª´ history  
4. L·∫•y `true_prices = df_future_true["close"].iloc[:horizon]`  
5. T√≠nh:

```python
   mse_path = mean((preds - true_prices)^2)
```

6. In ra:

   * Path MSE
   * Range gi√° th·∫≠t vs range gi√° d·ª± b√°o

ƒê√¢y l√† n∆°i b·∫°n s·∫Ω ghi **MSE 100 ng√†y tr√™n validation**.

---

# 6. D·ª± b√°o 100 ng√†y cu·ªëi c√πng v√† t·∫°o submission

## 6.1. D·ª± b√°o 100 ng√†y t·ª´ to√†n b·ªô l·ªãch s·ª≠

Sau khi ƒë√£ tin t∆∞·ªüng pipeline, m√¥ h√¨nh cu·ªëi c√πng ƒë∆∞·ª£c d√πng ƒë·ªÉ d·ª± b√°o t∆∞∆°ng lai th·ª±c t·∫ø cho submission:

```python
preds_future = forecast_future_prices(
    df_hist_raw=df,
    trend_model=trend_model,
    residual_model=final_residual_model,
    feature_names=top_features,
    steps=FORECAST_STEPS,
    log_clip_low=clip_low,
    log_clip_high=clip_high,
    residual_shrink=RESIDUAL_SHRINK,
)
```

·ªû ƒë√¢y:

* `df_hist_raw` ch√≠nh l√† full l·ªãch s·ª≠ FPT c√≥ s·∫µn trong train
* `FORECAST_STEPS = 100`
* Output `preds_future` l√† m·∫£ng 100 gi√° `close` d·ª± b√°o cho 100 ng√†y ti·∫øp theo. 

---

## 6.2. Gh√©p v·ªõi template v√† xu·∫•t submission

Cu·ªëi c√πng, pipeline ƒë·ªçc template submission:

1. ƒê·ªçc `sub_template = pd.read_csv(SUBMISSION_TEMPLATE_CSV)`

2. Ki·ªÉm tra `len(sub_template) == FORECAST_STEPS`

3. T·∫°o b·∫£n copy:

   ```python
   submission = sub_template.copy()
   submission["close"] = preds_future.astype(float)
   submission.to_csv(OUTPUT_SUBMISSION_CSV, index=False)
   ```

4. In m·ªôt v√†i d√≤ng ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra, v√† th√¥ng b√°o ƒë∆∞·ªùng d·∫´n file submission. 

---

# PH·∫¶N CU·ªêI: FORMAT CHO K·∫æT QU·∫¢ SUBMISSION

D∆∞·ªõi ƒë√¢y l√† khung b·∫°n c√≥ th·ªÉ d√πng trong report ho·∫∑c notebook ƒë·ªÉ tr√¨nh b√†y k·∫øt qu·∫£, bao g·ªìm ch·ªó tr·ªëng cho MSE 100 ng√†y.

---

## üîπ 1. File submission

File `submission.csv` g·ªìm c√°c c·ªôt gi·ªëng template c·ªßa BTC, trong ƒë√≥ c·ªôt `close` ƒë∆∞·ª£c thay b·∫±ng d·ª± b√°o 100 ng√†y:

| id | close |
| ----- | -------------- |
| 0     | 116.051314     |
| 1     | 113.060426     |
| ...   | ...            |
| 99    | 17.667         |


---

## üîπ 2. K·∫øt qu·∫£ ki·ªÉm tra 100 ng√†y tr√™n Leaderboard

```python
MSE: 5406.9571 
```
---

